Q.1 The vendor of the attack target is concerned that news of this attack
    could scare off potential customers; they will make any alteration
    necessary (in software or hardware) to prevent the attack.  Explain the
    options they have, and which one you would recommend.

A.1 Ultimately this attack feeds off of leaked information, specifically that of
    the reduction in the RSA square and multiply step.  Thus to prevent this
    target from being attacked, the implementation would have to not leak any
    information regarding the computation.

    The attack we implemented exploits the time taken to compute an iteration of
    square and multiply dependent on the key bit/whether or not the intermediate
    value was reduced.  Thus to protect from my attack implementation the
    computation done by the target would have to be done in constant (or near
    constant) time.  This could be achieved through either software or hardware
    approaches.

    A software approach could involve many different tricks.  In the case of
    RSA in this target, a simple trick to throw off the adversary would be to
    add some form of random computation in the computation step.  This would
    cause even the same computation to take random time, making the timing data
    unusable.  An example of this would be exponent blinding, which adds a
    random term to the exponent that is cancelled out in the encryption process.
    If the target were to use a different cryptographic procedure, trying to
    randomise the order of computation would be a good move as well, however
    this is not feasible/possible with RSA.  The drawback with implementing
    constant-time implementations is that they incur some overhead in time (and
    possibly memory) requirements.

    A hardware approach could involve a few different approaches, the simplest
    of which being the us of an integrated circuit/processor that performs the
    operation's for a square and multiply in this case, or whatever the required
    operation in the cryptographic scheme may be, in constant time.  The
    software implementation would of course need to utilise any specific
    instructions (if any) in order to take advantage of this, but the resulting
    implementation would then be in theory not leak any information regarding
    whether or not a reduction occurred.

================================================================================

Q.2 The vendor of the attack target is concerned that a similar attack may
    also apply to their implementation of ElGamal encryption.  Explain the
    potential for such an attack, and how it differs wrt.  the case of RSA.

A.2 ElGamal decryption uses some of the same mathematical procedures of RSA.
    Although the encryption process uses some randomly chosen information, this
    only acts as a message blinding technique.  On the other hand, the
    decryption process only makes use of the secret key, x, and the two values
    in the ciphertext pairs.  If the implementation uses the same square and
    multiply technique as used by the RSA implementation, it would thus be
    susceptible to the same attack.

================================================================================

Q.3 Your implementation of this attack will use some form of statistical
    test.  Explain what test you have used, how it works, and also how it
    compares with at least one alternative.

A.3 My implementation uses quite a naive statistical test for checking what a
    bit is, by comparing the means of the different subsets of timing data.
    As all the sample ciphertexts are calculated locally, the result on whether
    or not they are reduced splits them into different subsets.  The timing
    information associated with the messages are then, by definition also sorted
    into sets.  I average the times in each of the sets, and then work out some
    inequalities:

    F_mu[X] is the average time for data set X.
    F_mu[X] corresponds to F[X+1] as labelled in the paper.

    diff1 = abs(F_mu[0] - F_mu[1])
	diff2 = abs(F_mu[2] - F_mu[3])
	k1_lt = F_mu[0] > F_mu[1]
	k1_eq = abs(1 - (F_mu[0] / F_mu[1])) < threshold
	k0_lt = F_mu[2] > F_mu[3]
	k0_eq = abs(1 - (F_mu[2] / F_mu[3])) < threshold

    After working out the preceding boolean values. First diff1 is compared with
    diff2.  If diff1 > diff2 we know that it is likely that F_mu[2] and F_mu[3]
    are closer together, perhaps even approximately equal as the paper suggests.
    This suggests that the bit should be a 1.  In the case where diff2 > diff1,
    the opposite is suggested, and the bit appears that it should be a 0.  After
    establishing this, I also check the inequalities provided by the paper.
    These variable are named accordingly:

    k1    - for checks if the bit is a 1
    k0    - for checks if the bit is a 0
       lt - for the F_mu[N] >  F_mu[N+1] check
       eq - for the F_mu[N] ~= F_mu[N+1] check.

    These checks are also applied with the diff checks, meaning that if we pass
    all the checks for a 0/1 bit case, we can be fairly certain that this is in
    fact the bit we are looking for.  However, should any of these checks fail,
    there is some ambiguity/error to what is expected in the mathematics, and
    thus and error is returned.

    This statistical test is reasonably accurate, however can be "tricked" by
    improper timing data.  Be it from noise, deliberate error from the target
    or any other means, if the data is not accurate, erroneous results can
    repeatedly occur, causing the implementation to loop - or worse, errors
    could be falsely reported or not reported at all!

    An alternative method of statistical analysis could be calibrated to know
    the time taken for a calculation to occur, and how that corresponds to the
    internal implementation that is used to check whether a reduction occurs.
    Then the timing data observed from the target, could be correlated with the
    calibrated timing data achieved locally (assumed to be with little to no
    error).  At this point timing samples could be identified to be erroneous
    and thus discarded, allowing for the remainder of the statistical analysis
    to be carried our with much greater accuracy.

================================================================================

Q.8 Imagine you read a research paper that suggests a larger key (e.g.,
    2048-bit rather than 1024-bit) could help to prevent this attack.
    Explain whether and why you think this is right (or wrong).

A.8 I would describe this to be categorically wrong.  Increased key size will
    not prevent attack, just increase the amount of time taken for the attack to
    be carried out.  This is due to the fact that on any reasonably modern bit
    of hardware, with a reasonably optimised implementation, the runtime of the
    attack would not slow that drastically, even at these larger key sizes.

    At every iteration of the attack can run in roughly "constant" time.  By
    that I mean, no computation should need to be done that is linked to the key
    size.  In my implementation, if an error occurs the data has to be
    regenerated, which would take longer at larger key sizes.  But this is purely
    down to the implementation complexity/time spent to reward ratio (laziness),
    and not infeasible, a fully dynamic programming approach would be possible
    here that would solve this with acceptable memory usage.