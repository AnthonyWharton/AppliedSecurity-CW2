-------------------------------------------------------------------------------

1. Instructions

- replace any [...] with free text,
  and
- replace the [?] with an X if you have completed that stage,
- replace the [?] with an * if you have attempted that stage, but you know
  it doesn't work completely; document why you think it doesn't work, plus
  what you would do to fix the problem, at the end of the marksheet.

-------------------------------------------------------------------------------

2. Information

So that we can calibrate and improve the assignment in the future, give us
a rough idea how long (in hours) you spent on it *in total*:

effort : stage 1 -> 20 hours
effort : stage 2 -> 25 hours
effort : stage 3 -> [...] hours
effort : stage 4 -> [...] hours

-------------------------------------------------------------------------------

3. Citation

Clearly it might have an influence on your mark, but the use of third-party
resources *is* allowed *if* correctly cited (unless explicitly prohibited 
by the assignment description of course).  Let us know what third-party 
source code or resources you used (if any) so it's clear what's your work 
and what isn't:

Stage 1:
 - https://www.ietf.org/rfc/rfc2437.txt
 - https://www.ietf.org/rfc/rfc3447.txt
 - Manger, J., 2001, August. A chosen ciphertext attack on RSA optimal
   asymmetric encryption padding (OAEP) as standardized in PKCS# 1 v2. 0. In
   Annual International Cryptology Conference (pp. 230-238). Springer, Berlin,
   Heidelberg.

Stage 2:
 - Dhem, Jean-Francois, et al. "A practical implementation of the timing
   attack." International Conference on Smart Card Research and Advanced
   Applications. Springer, Berlin, Heidelberg, 1998.

-------------------------------------------------------------------------------

4. Marking

The following gives a stage-by-stage description of the assignment marking
scheme.  Note this acts as an indicative guideline only, including weights
for each more obvious aspect (e.g., functional correctness); other aspects
outside this list can warrant an increase/decrease in marks, with examples
including hard to quantify features such as the efficiency, robustness, 
generality, realism, or style of a solution.

[X] Stage 1 : an attack based on error messages
              - correct challenge material                           ( 40%)
              - solution quality wrt. efficiency, robustness etc.    ( 30%)
              - analysis questions                                   ( 30%)

              challenge material (i.e., plaintext  m^{*}) :
              6A2B052ADCCDD3A2DE7D1AB268F2EC5018B257A04C8ED56C896DA9

[X] Stage 2 : an attack based on execution time
              - correct challenge material                           ( 40%)
              - solution quality wrt. efficiency, robustness etc.    ( 30%)
              - analysis questions                                   ( 30%)

              challenge material (i.e., exponent   d^{*}) : 1660947EF60BBF7F

[?] Stage 3 : an attack based on an injected fault
              - correct challenge material                           ( 40%)
              - solution quality wrt. efficiency, robustness etc.    ( 30%)
              - analysis questions                                   ( 30%)

              challenge material (i.e., cipher key k^{*}) : [...]

[?] Stage 4 : an attack based on power consumption
              - correct challenge material                           ( 40%)
              - solution quality wrt. efficiency, robustness etc.    ( 30%)
              - analysis questions                                   ( 30%)

              challenge material (i.e., cipher key k^{*}) : [...]

                                                                     ------
                                                                     (400%)

-------------------------------------------------------------------------------

5. Documentation

Any other documentation, notes or comments that you think are important or
might be easy to overlook (e.g., a subtle issue or technique in associated
source code) should go here:

Stage 1:
    Expected Run Time:
        Around 5 seconds on lab machine with given test files.
    Notes:
        Nothing extraordinary on top of the paper/RFC docs, all documented
        in-file.

Stage 2:
    Expected Run Time:
        Tested on a lab machine, I get anywhere from around 15 seconds with no
        errors being picked up, to up to 45-60 seconds in unlucky cases with 15+
        errors. (With given test files.)
    Notes:
        Error checking was done by comparing the F1-F2 (diff1) and F3-F4 (diff2)
        differences, along with the inequality in the paper. This was by far the
        least effective part of the code, with occasional runs ending up in a
        state of looping errors. In order to combat this, I looked at adapting
        the roll-back code to take this into account.

        I kept an list of positions in the key, and the amount of times we have
        error'd there. This was stored in a recursive doubling approach, as if
        an error occurred at around the same area more than once, it was noted
        that often the error occurred earlier and rolling back further would be
        beneficial.

        On an error, I generated some more samples, looked at the location of
        the error in this array (as well as the bits around it as a sort of
        locality-check) and then rolled back that amount. I used some light
        dynamic programming between rounds, storing the values from the internal
        oracles and using them for the next iteration, however when rolling back
        I regenerated the values. Although it would not be out of reasonable
        bounds to store all previous iterations' computed values, I decided that
        the process of additionally regenerating the precomputed values when
        samples were added was beyond required scope, and would provide little
        additional benefit.


-------------------------------------------------------------------------------

